{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>artist</th>\n",
       "      <th>genre</th>\n",
       "      <th>rank</th>\n",
       "      <th>danceability</th>\n",
       "      <th>energy</th>\n",
       "      <th>key</th>\n",
       "      <th>loudness</th>\n",
       "      <th>mode</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>...</th>\n",
       "      <th>tempo</th>\n",
       "      <th>duration_ms</th>\n",
       "      <th>time_signature</th>\n",
       "      <th>lyrics</th>\n",
       "      <th>winter</th>\n",
       "      <th>spring</th>\n",
       "      <th>summer</th>\n",
       "      <th>autumn</th>\n",
       "      <th>id</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TOUT VA BIEN (feat. Ninho &amp; Naps)</td>\n",
       "      <td>Alonzo</td>\n",
       "      <td>francoton, french hip hop, pop urbaine, rap fr...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.660</td>\n",
       "      <td>0.720</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-5.874</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0753</td>\n",
       "      <td>...</td>\n",
       "      <td>99.937</td>\n",
       "      <td>192960.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>[Paroles de \"TOUT VA BIEN\" ft. Naps &amp; Ninho] [...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6OZwia8loN0aPS0vTvsBjR</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FADE UP</td>\n",
       "      <td>ZEG P</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>0.758</td>\n",
       "      <td>0.599</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-5.990</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0877</td>\n",
       "      <td>...</td>\n",
       "      <td>129.960</td>\n",
       "      <td>219188.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>[Paroles de \"FADE UP\" ft. Hamza &amp; SCH] [Intro ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4ZpIuzx91EAPK3VimONbfB</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DIE</td>\n",
       "      <td>Gazo</td>\n",
       "      <td>drill francais, rap francais</td>\n",
       "      <td>3</td>\n",
       "      <td>0.695</td>\n",
       "      <td>0.630</td>\n",
       "      <td>8.0</td>\n",
       "      <td>-7.160</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0350</td>\n",
       "      <td>...</td>\n",
       "      <td>130.968</td>\n",
       "      <td>240413.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>[Paroles de \"DIE\"] [Intro] La mala est gangx E...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3D29kjUyWxsT3jUUTtARVQ</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PETETE</td>\n",
       "      <td>Gambi</td>\n",
       "      <td>french hip hop, pop urbaine, rap francais</td>\n",
       "      <td>4</td>\n",
       "      <td>0.752</td>\n",
       "      <td>0.669</td>\n",
       "      <td>10.0</td>\n",
       "      <td>-9.817</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2510</td>\n",
       "      <td>...</td>\n",
       "      <td>155.997</td>\n",
       "      <td>123846.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>[Paroles de \"PETETE\"] [Intro] Pew Grr Pew pew ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0z3bi63SNZ5ylyHOzb81Uq</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Time Time</td>\n",
       "      <td>Trei Degete</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>0.715</td>\n",
       "      <td>0.799</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-6.447</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0433</td>\n",
       "      <td>...</td>\n",
       "      <td>128.925</td>\n",
       "      <td>156373.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>[Paroles de \"Time Time\"] [Couplet 1 : Squeezie...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5wKDPtbdggE1roeVp3UdXX</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                               title       artist  \\\n",
       "0  TOUT VA BIEN (feat. Ninho & Naps)       Alonzo   \n",
       "1                            FADE UP        ZEG P   \n",
       "2                                DIE         Gazo   \n",
       "3                             PETETE        Gambi   \n",
       "4                          Time Time  Trei Degete   \n",
       "\n",
       "                                               genre  rank  danceability  \\\n",
       "0  francoton, french hip hop, pop urbaine, rap fr...     1         0.660   \n",
       "1                                                NaN     2         0.758   \n",
       "2                       drill francais, rap francais     3         0.695   \n",
       "3          french hip hop, pop urbaine, rap francais     4         0.752   \n",
       "4                                                NaN     5         0.715   \n",
       "\n",
       "   energy   key  loudness  mode  speechiness  ...    tempo  duration_ms  \\\n",
       "0   0.720   3.0    -5.874   0.0       0.0753  ...   99.937     192960.0   \n",
       "1   0.599   4.0    -5.990   0.0       0.0877  ...  129.960     219188.0   \n",
       "2   0.630   8.0    -7.160   0.0       0.0350  ...  130.968     240413.0   \n",
       "3   0.669  10.0    -9.817   0.0       0.2510  ...  155.997     123846.0   \n",
       "4   0.799   0.0    -6.447   0.0       0.0433  ...  128.925     156373.0   \n",
       "\n",
       "   time_signature                                             lyrics  winter  \\\n",
       "0             4.0  [Paroles de \"TOUT VA BIEN\" ft. Naps & Ninho] [...       0   \n",
       "1             4.0  [Paroles de \"FADE UP\" ft. Hamza & SCH] [Intro ...       0   \n",
       "2             4.0  [Paroles de \"DIE\"] [Intro] La mala est gangx E...       0   \n",
       "3             4.0  [Paroles de \"PETETE\"] [Intro] Pew Grr Pew pew ...       0   \n",
       "4             4.0  [Paroles de \"Time Time\"] [Couplet 1 : Squeezie...       0   \n",
       "\n",
       "   spring  summer autumn                      id  type  \n",
       "0       1       0      0  6OZwia8loN0aPS0vTvsBjR     0  \n",
       "1       0       1      0  4ZpIuzx91EAPK3VimONbfB     0  \n",
       "2       0       1      0  3D29kjUyWxsT3jUUTtARVQ     0  \n",
       "3       0       1      0  0z3bi63SNZ5ylyHOzb81Uq     0  \n",
       "4       0       0      1  5wKDPtbdggE1roeVp3UdXX     0  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top = pd.read_csv('top200.csv')\n",
    "bottom = pd.read_csv('bottom200.csv')\n",
    "top['type'] = 0 # 0 for top\n",
    "bottom['type'] = 1 # 1 for bottom\n",
    "df = pd.concat([top, bottom], ignore_index=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>danceability</th>\n",
       "      <th>energy</th>\n",
       "      <th>key</th>\n",
       "      <th>loudness</th>\n",
       "      <th>mode</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>liveness</th>\n",
       "      <th>valence</th>\n",
       "      <th>tempo</th>\n",
       "      <th>duration_ms</th>\n",
       "      <th>time_signature</th>\n",
       "      <th>winter</th>\n",
       "      <th>spring</th>\n",
       "      <th>summer</th>\n",
       "      <th>autumn</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.660</td>\n",
       "      <td>0.720</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-5.874</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0753</td>\n",
       "      <td>0.3010</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.2260</td>\n",
       "      <td>0.628</td>\n",
       "      <td>99.937</td>\n",
       "      <td>192960.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.758</td>\n",
       "      <td>0.599</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-5.990</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0877</td>\n",
       "      <td>0.5700</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.1290</td>\n",
       "      <td>0.557</td>\n",
       "      <td>129.960</td>\n",
       "      <td>219188.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.695</td>\n",
       "      <td>0.630</td>\n",
       "      <td>8.0</td>\n",
       "      <td>-7.160</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0350</td>\n",
       "      <td>0.2290</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1180</td>\n",
       "      <td>0.550</td>\n",
       "      <td>130.968</td>\n",
       "      <td>240413.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.752</td>\n",
       "      <td>0.669</td>\n",
       "      <td>10.0</td>\n",
       "      <td>-9.817</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2510</td>\n",
       "      <td>0.1670</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0795</td>\n",
       "      <td>0.627</td>\n",
       "      <td>155.997</td>\n",
       "      <td>123846.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.715</td>\n",
       "      <td>0.799</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-6.447</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0433</td>\n",
       "      <td>0.0141</td>\n",
       "      <td>0.003500</td>\n",
       "      <td>0.3420</td>\n",
       "      <td>0.916</td>\n",
       "      <td>128.925</td>\n",
       "      <td>156373.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   danceability  energy   key  loudness  mode  speechiness  acousticness  \\\n",
       "0         0.660   0.720   3.0    -5.874   0.0       0.0753        0.3010   \n",
       "1         0.758   0.599   4.0    -5.990   0.0       0.0877        0.5700   \n",
       "2         0.695   0.630   8.0    -7.160   0.0       0.0350        0.2290   \n",
       "3         0.752   0.669  10.0    -9.817   0.0       0.2510        0.1670   \n",
       "4         0.715   0.799   0.0    -6.447   0.0       0.0433        0.0141   \n",
       "\n",
       "   instrumentalness  liveness  valence    tempo  duration_ms  time_signature  \\\n",
       "0          0.000000    0.2260    0.628   99.937     192960.0             4.0   \n",
       "1          0.000001    0.1290    0.557  129.960     219188.0             4.0   \n",
       "2          0.000000    0.1180    0.550  130.968     240413.0             4.0   \n",
       "3          0.000000    0.0795    0.627  155.997     123846.0             4.0   \n",
       "4          0.003500    0.3420    0.916  128.925     156373.0             4.0   \n",
       "\n",
       "   winter  spring  summer  autumn  type  \n",
       "0       0       1       0       0     0  \n",
       "1       0       0       1       0     0  \n",
       "2       0       0       1       0     0  \n",
       "3       0       0       1       0     0  \n",
       "4       0       0       0       1     0  "
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.drop(['title', 'artist', 'rank', 'lyrics', 'genre', 'id'], axis = 1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1423, 474, 475)"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = df.pop('type')\n",
    "x = df.values #returns a numpy array\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "x_scaled = min_max_scaler.fit_transform(x)\n",
    "X = pd.DataFrame(x_scaled)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.4, random_state=42)\n",
    "X_vali, X_test, y_vali, y_test = train_test_split(X_test, y_test, test_size=0.5, random_state=42)\n",
    "\n",
    "len(X_train), len(X_vali), len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1423, 17, 1)"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Uncomment for CNN model\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "X_test = np.array(X_test)\n",
    "X_vali = np.array(X_vali)\n",
    "X_train = X_train.reshape(X_train.shape[0],  X_train.shape[1], 1)\n",
    "X_test = X_test.reshape(X_test.shape[0],  X_test.shape[1], 1)\n",
    "X_vali = X_vali.reshape(X_vali.shape[0],  X_vali.shape[1], 1)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_15\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d_33 (Conv1D)          (None, 17, 64)            256       \n",
      "                                                                 \n",
      " max_pooling1d_33 (MaxPoolin  (None, 8, 64)            0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " dropout_9 (Dropout)         (None, 8, 64)             0         \n",
      "                                                                 \n",
      " conv1d_34 (Conv1D)          (None, 8, 32)             6176      \n",
      "                                                                 \n",
      " max_pooling1d_34 (MaxPoolin  (None, 4, 32)            0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " dropout_10 (Dropout)        (None, 4, 32)             0         \n",
      "                                                                 \n",
      " conv1d_35 (Conv1D)          (None, 4, 16)             1552      \n",
      "                                                                 \n",
      " max_pooling1d_35 (MaxPoolin  (None, 2, 16)            0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " dropout_11 (Dropout)        (None, 2, 16)             0         \n",
      "                                                                 \n",
      " flatten_11 (Flatten)        (None, 32)                0         \n",
      "                                                                 \n",
      " dense_42 (Dense)            (None, 64)                2112      \n",
      "                                                                 \n",
      " dense_43 (Dense)            (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 10,161\n",
      "Trainable params: 10,161\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# CNN Model\n",
    "\n",
    "model = keras.Sequential([\n",
    "  layers.Conv1D(64, (3), activation='relu', padding='same', input_shape=(X_train.shape[1], 1 )),\n",
    "  layers.MaxPooling1D((2)),\n",
    "  layers.Dropout(0.2),\n",
    "  layers.Conv1D(32, (3), activation='relu', padding='same'),\n",
    "  layers.MaxPooling1D((2)),\n",
    "  layers.Dropout(0.2),\n",
    "  layers.Conv1D(16, (3), activation='relu', padding='same'),\n",
    "  layers.MaxPooling1D((2)),\n",
    "  layers.Dropout(0.2),\n",
    "  layers.Flatten(),\n",
    "  layers.Dense(64, activation='relu'),\n",
    "  layers.Dense(1, activation='softmax')\n",
    "])\n",
    "model.summary()\n",
    "\n",
    "# Fully Connected Model\n",
    "\n",
    "# model = keras.Sequential([\n",
    "#   layers.Dense(256, activation='relu', kernel_initializer='he_normal', input_shape=(X_train.shape[1],)),\n",
    "#   layers.Dense(128, activation='relu', kernel_initializer='he_normal'),\n",
    "#   layers.Dense(64, activation='relu', kernel_initializer='he_normal'),\n",
    "#   layers.Dense(32, activation='relu', kernel_initializer='he_normal'),\n",
    "#   layers.Dense(1, activation='sigmoid'),\n",
    "# ])\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "      optimizer=keras.optimizers.Adam(clipvalue=0.5), \n",
    "      loss='binary_crossentropy',\n",
    "      metrics=['accuracy']\n",
    "      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "12/12 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.5538 - val_loss: nan - val_accuracy: 0.5506\n",
      "Epoch 2/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5538 - val_loss: nan - val_accuracy: 0.5506\n",
      "Epoch 3/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5538 - val_loss: nan - val_accuracy: 0.5506\n",
      "Epoch 4/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5538 - val_loss: nan - val_accuracy: 0.5506\n",
      "Epoch 5/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5538 - val_loss: nan - val_accuracy: 0.5506\n",
      "Epoch 6/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5538 - val_loss: nan - val_accuracy: 0.5506\n",
      "Epoch 7/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5538 - val_loss: nan - val_accuracy: 0.5506\n",
      "Epoch 8/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5538 - val_loss: nan - val_accuracy: 0.5506\n",
      "Epoch 9/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5538 - val_loss: nan - val_accuracy: 0.5506\n",
      "Epoch 10/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5538 - val_loss: nan - val_accuracy: 0.5506\n",
      "Epoch 11/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5538 - val_loss: nan - val_accuracy: 0.5506\n",
      "Epoch 12/200\n",
      "12/12 [==============================] - 0s 6ms/step - loss: nan - accuracy: 0.5538 - val_loss: nan - val_accuracy: 0.5506\n",
      "Epoch 13/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5538 - val_loss: nan - val_accuracy: 0.5506\n",
      "Epoch 14/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5538 - val_loss: nan - val_accuracy: 0.5506\n",
      "Epoch 15/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5538 - val_loss: nan - val_accuracy: 0.5506\n",
      "Epoch 16/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5538 - val_loss: nan - val_accuracy: 0.5506\n",
      "Epoch 17/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5538 - val_loss: nan - val_accuracy: 0.5506\n",
      "Epoch 18/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5538 - val_loss: nan - val_accuracy: 0.5506\n",
      "Epoch 19/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5538 - val_loss: nan - val_accuracy: 0.5506\n",
      "Epoch 20/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5538 - val_loss: nan - val_accuracy: 0.5506\n",
      "Epoch 21/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5538 - val_loss: nan - val_accuracy: 0.5506\n",
      "Epoch 22/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5538 - val_loss: nan - val_accuracy: 0.5506\n",
      "Epoch 23/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5538 - val_loss: nan - val_accuracy: 0.5506\n",
      "Epoch 24/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5538 - val_loss: nan - val_accuracy: 0.5506\n",
      "Epoch 25/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5538 - val_loss: nan - val_accuracy: 0.5506\n",
      "Epoch 26/200\n",
      "12/12 [==============================] - 0s 6ms/step - loss: nan - accuracy: 0.5538 - val_loss: nan - val_accuracy: 0.5506\n",
      "Epoch 27/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5538 - val_loss: nan - val_accuracy: 0.5506\n",
      "Epoch 28/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5538 - val_loss: nan - val_accuracy: 0.5506\n",
      "Epoch 29/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5538 - val_loss: nan - val_accuracy: 0.5506\n",
      "Epoch 30/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5538 - val_loss: nan - val_accuracy: 0.5506\n",
      "Epoch 31/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5538 - val_loss: nan - val_accuracy: 0.5506\n",
      "Epoch 32/200\n",
      "12/12 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.5538 - val_loss: nan - val_accuracy: 0.5506\n",
      "Epoch 33/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5538 - val_loss: nan - val_accuracy: 0.5506\n",
      "Epoch 34/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5538 - val_loss: nan - val_accuracy: 0.5506\n",
      "Epoch 35/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5538 - val_loss: nan - val_accuracy: 0.5506\n",
      "Epoch 36/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5538 - val_loss: nan - val_accuracy: 0.5506\n",
      "Epoch 37/200\n",
      "12/12 [==============================] - 0s 6ms/step - loss: nan - accuracy: 0.5538 - val_loss: nan - val_accuracy: 0.5506\n",
      "Epoch 38/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5538 - val_loss: nan - val_accuracy: 0.5506\n",
      "Epoch 39/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5538 - val_loss: nan - val_accuracy: 0.5506\n",
      "Epoch 40/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5538 - val_loss: nan - val_accuracy: 0.5506\n",
      "Epoch 41/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5538 - val_loss: nan - val_accuracy: 0.5506\n",
      "Epoch 42/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5538 - val_loss: nan - val_accuracy: 0.5506\n",
      "Epoch 43/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5538 - val_loss: nan - val_accuracy: 0.5506\n",
      "Epoch 44/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5538 - val_loss: nan - val_accuracy: 0.5506\n",
      "Epoch 45/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5538 - val_loss: nan - val_accuracy: 0.5506\n",
      "Epoch 46/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5538 - val_loss: nan - val_accuracy: 0.5506\n",
      "Epoch 47/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5538 - val_loss: nan - val_accuracy: 0.5506\n",
      "Epoch 48/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5538 - val_loss: nan - val_accuracy: 0.5506\n",
      "Epoch 49/200\n",
      "12/12 [==============================] - 0s 6ms/step - loss: nan - accuracy: 0.5538 - val_loss: nan - val_accuracy: 0.5506\n",
      "Epoch 50/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5538 - val_loss: nan - val_accuracy: 0.5506\n",
      "Epoch 51/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5538 - val_loss: nan - val_accuracy: 0.5506\n",
      "Epoch 52/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5538 - val_loss: nan - val_accuracy: 0.5506\n",
      "Epoch 53/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5538 - val_loss: nan - val_accuracy: 0.5506\n",
      "Epoch 54/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5538 - val_loss: nan - val_accuracy: 0.5506\n",
      "Epoch 55/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5538 - val_loss: nan - val_accuracy: 0.5506\n",
      "Epoch 56/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5538 - val_loss: nan - val_accuracy: 0.5506\n",
      "Epoch 57/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5538 - val_loss: nan - val_accuracy: 0.5506\n",
      "Epoch 58/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5538 - val_loss: nan - val_accuracy: 0.5506\n",
      "Epoch 59/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5538 - val_loss: nan - val_accuracy: 0.5506\n",
      "Epoch 60/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5538 - val_loss: nan - val_accuracy: 0.5506\n",
      "Epoch 61/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5538 - val_loss: nan - val_accuracy: 0.5506\n",
      "Epoch 62/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5538 - val_loss: nan - val_accuracy: 0.5506\n",
      "Epoch 63/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5538 - val_loss: nan - val_accuracy: 0.5506\n",
      "Epoch 64/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5538 - val_loss: nan - val_accuracy: 0.5506\n",
      "Epoch 65/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5538 - val_loss: nan - val_accuracy: 0.5506\n",
      "Epoch 66/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5538 - val_loss: nan - val_accuracy: 0.5506\n",
      "Epoch 67/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5538 - val_loss: nan - val_accuracy: 0.5506\n",
      "Epoch 68/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5538 - val_loss: nan - val_accuracy: 0.5506\n",
      "Epoch 69/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5538 - val_loss: nan - val_accuracy: 0.5506\n",
      "Epoch 70/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5538 - val_loss: nan - val_accuracy: 0.5506\n",
      "Epoch 71/200\n",
      "12/12 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5538 - val_loss: nan - val_accuracy: 0.5506\n",
      "Epoch 72/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5538 - val_loss: nan - val_accuracy: 0.5506\n",
      "Epoch 73/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5538 - val_loss: nan - val_accuracy: 0.5506\n",
      "Epoch 74/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5538 - val_loss: nan - val_accuracy: 0.5506\n",
      "Epoch 75/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5538 - val_loss: nan - val_accuracy: 0.5506\n",
      "Epoch 76/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5538 - val_loss: nan - val_accuracy: 0.5506\n",
      "Epoch 77/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5538 - val_loss: nan - val_accuracy: 0.5506\n",
      "Epoch 78/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5538 - val_loss: nan - val_accuracy: 0.5506\n",
      "Epoch 79/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5538 - val_loss: nan - val_accuracy: 0.5506\n",
      "Epoch 80/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5538 - val_loss: nan - val_accuracy: 0.5506\n",
      "Epoch 81/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5538 - val_loss: nan - val_accuracy: 0.5506\n",
      "Epoch 82/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5538 - val_loss: nan - val_accuracy: 0.5506\n",
      "Epoch 83/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5538 - val_loss: nan - val_accuracy: 0.5506\n",
      "Epoch 84/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5538 - val_loss: nan - val_accuracy: 0.5506\n",
      "Epoch 85/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5538 - val_loss: nan - val_accuracy: 0.5506\n",
      "Epoch 86/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5538 - val_loss: nan - val_accuracy: 0.5506\n",
      "Epoch 87/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5538 - val_loss: nan - val_accuracy: 0.5506\n",
      "Epoch 88/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5538 - val_loss: nan - val_accuracy: 0.5506\n",
      "Epoch 89/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5538 - val_loss: nan - val_accuracy: 0.5506\n",
      "Epoch 90/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5538 - val_loss: nan - val_accuracy: 0.5506\n",
      "Epoch 91/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5538 - val_loss: nan - val_accuracy: 0.5506\n",
      "Epoch 92/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5538 - val_loss: nan - val_accuracy: 0.5506\n",
      "Epoch 93/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5538 - val_loss: nan - val_accuracy: 0.5506\n",
      "Epoch 94/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5538 - val_loss: nan - val_accuracy: 0.5506\n",
      "Epoch 95/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5538 - val_loss: nan - val_accuracy: 0.5506\n",
      "Epoch 96/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5538 - val_loss: nan - val_accuracy: 0.5506\n",
      "Epoch 97/200\n",
      "12/12 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.5538 - val_loss: nan - val_accuracy: 0.5506\n",
      "Epoch 98/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5538 - val_loss: nan - val_accuracy: 0.5506\n",
      "Epoch 99/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5538 - val_loss: nan - val_accuracy: 0.5506\n",
      "Epoch 100/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5538 - val_loss: nan - val_accuracy: 0.5506\n",
      "Epoch 101/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5538 - val_loss: nan - val_accuracy: 0.5506\n",
      "Epoch 102/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5538 - val_loss: nan - val_accuracy: 0.5506\n",
      "Epoch 103/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5538 - val_loss: nan - val_accuracy: 0.5506\n",
      "Epoch 104/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5538 - val_loss: nan - val_accuracy: 0.5506\n",
      "Epoch 105/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5538 - val_loss: nan - val_accuracy: 0.5506\n",
      "Epoch 106/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5538 - val_loss: nan - val_accuracy: 0.5506\n",
      "Epoch 107/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5538 - val_loss: nan - val_accuracy: 0.5506\n",
      "Epoch 108/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5538 - val_loss: nan - val_accuracy: 0.5506\n",
      "Epoch 109/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5538 - val_loss: nan - val_accuracy: 0.5506\n",
      "Epoch 110/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5538 - val_loss: nan - val_accuracy: 0.5506\n",
      "Epoch 111/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5538 - val_loss: nan - val_accuracy: 0.5506\n",
      "Epoch 112/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5538 - val_loss: nan - val_accuracy: 0.5506\n",
      "Epoch 113/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5538 - val_loss: nan - val_accuracy: 0.5506\n",
      "Epoch 114/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5538 - val_loss: nan - val_accuracy: 0.5506\n",
      "Epoch 115/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5538 - val_loss: nan - val_accuracy: 0.5506\n",
      "Epoch 116/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5538 - val_loss: nan - val_accuracy: 0.5506\n",
      "Epoch 117/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5538 - val_loss: nan - val_accuracy: 0.5506\n",
      "Epoch 118/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5538 - val_loss: nan - val_accuracy: 0.5506\n",
      "Epoch 119/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5538 - val_loss: nan - val_accuracy: 0.5506\n",
      "Epoch 120/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5538 - val_loss: nan - val_accuracy: 0.5506\n",
      "Epoch 121/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5538 - val_loss: nan - val_accuracy: 0.5506\n",
      "Epoch 122/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5538 - val_loss: nan - val_accuracy: 0.5506\n",
      "Epoch 123/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5538 - val_loss: nan - val_accuracy: 0.5506\n",
      "Epoch 124/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5538 - val_loss: nan - val_accuracy: 0.5506\n",
      "Epoch 125/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5538 - val_loss: nan - val_accuracy: 0.5506\n",
      "Epoch 126/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5538 - val_loss: nan - val_accuracy: 0.5506\n",
      "Epoch 127/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5538 - val_loss: nan - val_accuracy: 0.5506\n",
      "Epoch 128/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5538 - val_loss: nan - val_accuracy: 0.5506\n",
      "Epoch 129/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5538 - val_loss: nan - val_accuracy: 0.5506\n",
      "Epoch 130/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5538 - val_loss: nan - val_accuracy: 0.5506\n",
      "Epoch 131/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5538 - val_loss: nan - val_accuracy: 0.5506\n",
      "Epoch 132/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5538 - val_loss: nan - val_accuracy: 0.5506\n",
      "Epoch 133/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5538 - val_loss: nan - val_accuracy: 0.5506\n",
      "Epoch 134/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5538 - val_loss: nan - val_accuracy: 0.5506\n",
      "Epoch 135/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5538 - val_loss: nan - val_accuracy: 0.5506\n",
      "Epoch 136/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5538 - val_loss: nan - val_accuracy: 0.5506\n",
      "Epoch 137/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5538 - val_loss: nan - val_accuracy: 0.5506\n",
      "Epoch 138/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5538 - val_loss: nan - val_accuracy: 0.5506\n",
      "Epoch 139/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5538 - val_loss: nan - val_accuracy: 0.5506\n",
      "Epoch 140/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5538 - val_loss: nan - val_accuracy: 0.5506\n",
      "Epoch 141/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5538 - val_loss: nan - val_accuracy: 0.5506\n",
      "Epoch 142/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5538 - val_loss: nan - val_accuracy: 0.5506\n",
      "Epoch 143/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5538 - val_loss: nan - val_accuracy: 0.5506\n",
      "Epoch 144/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5538 - val_loss: nan - val_accuracy: 0.5506\n",
      "Epoch 145/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5538 - val_loss: nan - val_accuracy: 0.5506\n",
      "Epoch 146/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5538 - val_loss: nan - val_accuracy: 0.5506\n",
      "Epoch 147/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5538 - val_loss: nan - val_accuracy: 0.5506\n",
      "Epoch 148/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5538 - val_loss: nan - val_accuracy: 0.5506\n",
      "Epoch 149/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5538 - val_loss: nan - val_accuracy: 0.5506\n",
      "Epoch 150/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5538 - val_loss: nan - val_accuracy: 0.5506\n",
      "Epoch 151/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5538 - val_loss: nan - val_accuracy: 0.5506\n",
      "Epoch 152/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5538 - val_loss: nan - val_accuracy: 0.5506\n",
      "Epoch 153/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5538 - val_loss: nan - val_accuracy: 0.5506\n",
      "Epoch 154/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5538 - val_loss: nan - val_accuracy: 0.5506\n",
      "Epoch 155/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5538 - val_loss: nan - val_accuracy: 0.5506\n",
      "Epoch 156/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5538 - val_loss: nan - val_accuracy: 0.5506\n",
      "Epoch 157/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5538 - val_loss: nan - val_accuracy: 0.5506\n",
      "Epoch 158/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5538 - val_loss: nan - val_accuracy: 0.5506\n",
      "Epoch 159/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5538 - val_loss: nan - val_accuracy: 0.5506\n",
      "Epoch 160/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5538 - val_loss: nan - val_accuracy: 0.5506\n",
      "Epoch 161/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5538 - val_loss: nan - val_accuracy: 0.5506\n",
      "Epoch 162/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5538 - val_loss: nan - val_accuracy: 0.5506\n",
      "Epoch 163/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5538 - val_loss: nan - val_accuracy: 0.5506\n",
      "Epoch 164/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5538 - val_loss: nan - val_accuracy: 0.5506\n",
      "Epoch 165/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5538 - val_loss: nan - val_accuracy: 0.5506\n",
      "Epoch 166/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5538 - val_loss: nan - val_accuracy: 0.5506\n",
      "Epoch 167/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5538 - val_loss: nan - val_accuracy: 0.5506\n",
      "Epoch 168/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5538 - val_loss: nan - val_accuracy: 0.5506\n",
      "Epoch 169/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5538 - val_loss: nan - val_accuracy: 0.5506\n",
      "Epoch 170/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5538 - val_loss: nan - val_accuracy: 0.5506\n",
      "Epoch 171/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5538 - val_loss: nan - val_accuracy: 0.5506\n",
      "Epoch 172/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5538 - val_loss: nan - val_accuracy: 0.5506\n",
      "Epoch 173/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5538 - val_loss: nan - val_accuracy: 0.5506\n",
      "Epoch 174/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5538 - val_loss: nan - val_accuracy: 0.5506\n",
      "Epoch 175/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5538 - val_loss: nan - val_accuracy: 0.5506\n",
      "Epoch 176/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5538 - val_loss: nan - val_accuracy: 0.5506\n",
      "Epoch 177/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5538 - val_loss: nan - val_accuracy: 0.5506\n",
      "Epoch 178/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5538 - val_loss: nan - val_accuracy: 0.5506\n",
      "Epoch 179/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5538 - val_loss: nan - val_accuracy: 0.5506\n",
      "Epoch 180/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5538 - val_loss: nan - val_accuracy: 0.5506\n",
      "Epoch 181/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5538 - val_loss: nan - val_accuracy: 0.5506\n",
      "Epoch 182/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5538 - val_loss: nan - val_accuracy: 0.5506\n",
      "Epoch 183/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5538 - val_loss: nan - val_accuracy: 0.5506\n",
      "Epoch 184/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5538 - val_loss: nan - val_accuracy: 0.5506\n",
      "Epoch 185/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5538 - val_loss: nan - val_accuracy: 0.5506\n",
      "Epoch 186/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5538 - val_loss: nan - val_accuracy: 0.5506\n",
      "Epoch 187/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5538 - val_loss: nan - val_accuracy: 0.5506\n",
      "Epoch 188/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5538 - val_loss: nan - val_accuracy: 0.5506\n",
      "Epoch 189/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5538 - val_loss: nan - val_accuracy: 0.5506\n",
      "Epoch 190/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5538 - val_loss: nan - val_accuracy: 0.5506\n",
      "Epoch 191/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5538 - val_loss: nan - val_accuracy: 0.5506\n",
      "Epoch 192/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5538 - val_loss: nan - val_accuracy: 0.5506\n",
      "Epoch 193/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5538 - val_loss: nan - val_accuracy: 0.5506\n",
      "Epoch 194/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5538 - val_loss: nan - val_accuracy: 0.5506\n",
      "Epoch 195/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5538 - val_loss: nan - val_accuracy: 0.5506\n",
      "Epoch 196/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5538 - val_loss: nan - val_accuracy: 0.5506\n",
      "Epoch 197/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5538 - val_loss: nan - val_accuracy: 0.5506\n",
      "Epoch 198/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5538 - val_loss: nan - val_accuracy: 0.5506\n",
      "Epoch 199/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5538 - val_loss: nan - val_accuracy: 0.5506\n",
      "Epoch 200/200\n",
      "12/12 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.5538 - val_loss: nan - val_accuracy: 0.5506\n"
     ]
    }
   ],
   "source": [
    "earlyStop = keras.callbacks.EarlyStopping(monitor='val_accuracy',\n",
    "                                             patience=20)\n",
    "\n",
    "results = model.fit(X_train, y_train, \n",
    "                  batch_size=128,\n",
    "                  epochs=200, \n",
    "                  validation_data=(X_vali, y_vali), \n",
    "                  callbacks=[earlyStop]\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/15 [==============================] - 0s 895us/step - loss: nan - accuracy: 0.5705\n",
      "Accuracy: 0.571\n"
     ]
    }
   ],
   "source": [
    "loss, acc = model.evaluate(X_test, y_test)\n",
    "print(f\"Accuracy: {acc:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x2723974a770>"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1440x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting the loss and accuracy of the model\n",
    "fig, axis = plt.subplots(1, 2, figsize=(20, 5))\n",
    "axis[0].plot(results.history[\"loss\"], color='r', label = 'train loss')\n",
    "axis[0].plot(results.history[\"val_loss\"], color='b', label = 'val loss')\n",
    "axis[0].set_title('Loss Comparison')\n",
    "axis[0].legend()\n",
    "axis[1].plot(results.history[\"accuracy\"], color='r', label = 'train accuracy')\n",
    "axis[1].plot(results.history[\"val_accuracy\"], color='b', label = 'val accuracy')\n",
    "axis[1].set_title('Accuracy Comparison')\n",
    "axis[1].legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b9d3afb3205ed3a5dcda4b5da3d178cff17aece66e80fca6d05330ad459cba38"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
